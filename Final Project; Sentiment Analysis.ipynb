{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "## the runtime error message is unimportant; it appears in every tensorflow usage in python3.6\n",
    "ops.reset_default_graph()\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "### Inspired from https://github.com/luhego/udacity-deep-learning/tree/master/sentiment-analysis with personal modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "## reads the csv file in pandas dataset format\n",
    "print(type(reviews))\n",
    "print(reviews[0][0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  74074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_counts = Counter()\n",
    "## creates a list of words. each word has a place in 2d array, each place records how many times the word was used\n",
    "for idx, row in reviews.iterrows():\n",
    "    review = row[0]\n",
    "    for word in review.split(' '):\n",
    "        total_counts[word] += 1\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major modifications done to data set from Udacity code to decrease variance of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'an', 'who', 'so', 'from', 'like', 'there', 'her', 'or', 'just', 'about', 'out', 'if', 'has', 'what', 'some', 'good', 'can', 'more', 'she', 'when', 'very', 'up', 'time', 'no', 'even', 'my', 'would', 'which', 'story', 'only', 'really', 'see', 'their', 'had', 'we', 'were', 'me', 'well', 'than', 'much', 'get', 'bad', 'been', 'people', 'will', 'do', 'other', 'also', 'into']\n",
      "good 15143\n",
      "layman :  9\n"
     ]
    }
   ],
   "source": [
    "## gets the 20000 most frequently used words excluding unnecessary ones\n",
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[35:20035]\n",
    "print(vocab[:50])\n",
    "print(\"good\", total_counts[\"good\"])\n",
    "\n",
    "## last word in 10000 word data set is used 9 times, as seen from the print statements\n",
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## creates a dictionary indicating what the index of the word is in the vocab list (one with 20000 most frequent words)\n",
    "word2idx = {word: index for index, word in enumerate(vocab)}## create the word-to-index dictionary here\n",
    "\n",
    "## converts a text (string) into a vector form \n",
    "def text_to_vector(text):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for word in text.split(' '):\n",
    "        index = word2idx.get(word, None)\n",
    "            ##.get method returns None when word cannot be found\n",
    "        if index:\n",
    "            vector[index] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## so the input layer is going to be: each review is a 2d word vector, layer is 2d matrix with a review in each row\n",
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)\n",
    "for ii, (_, text) in enumerate(reviews.iterrows()):\n",
    "    word_vectors[ii] = text_to_vector(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 2, 6, 0, 2, 5, 0, 2, 8, 1, 1, 0, 4, 2, 5, 0, 0, 3, 1, 0, 2, 0, 1],\n",
       "       [0, 3, 2, 3, 0, 2, 4, 1, 7, 4, 2, 6, 4, 3, 0, 0, 1, 2, 2, 0, 2, 0, 2],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the first 5 word vectors to check correct execution\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of inspired code from Udacity dataset\n",
    "# Dividing the dataset into training set and dev/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepares data to fit the dimensions required by the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## number of reviews\n",
    "reviewNum = len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 23750)\n",
      "(1, 23750)\n"
     ]
    }
   ],
   "source": [
    "## train_fraction shows percentage of training set from the entire data set\n",
    "train_fraction = 0.95\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "trainNum = int(np.rint(reviewNum * train_fraction))\n",
    "# devNum = int((reviewNum - trainNum) / 2)\n",
    "normalize_max = np.amax(word_vectors)\n",
    "#word_vectors = word_vectors / normalize_max\n",
    "\n",
    "trainX, trainY = word_vectors[:trainNum].T, Y[:trainNum].T\n",
    "# devX, devY = word_vectors[trainNum:(trainNum+devNum)].T, Y[trainNum:(trainNum+devNum)].T\n",
    "# testX, testY = word_vectors[trainNum+devNum:].T, Y[trainNum+devNum:].T\n",
    "testX, testY = word_vectors[trainNum:].T, Y[trainNum:].T\n",
    "\n",
    "trainX = trainX.astype(np.float64)\n",
    "\n",
    "## shows the shape of training set for inputs and labels\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts the label in to one-hot label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_one_hot(labels, C):\n",
    "    labels = np.squeeze(np.asarray(labels))\n",
    "    ## constant shows how many classes to predict\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    ## tensor for converting to one hot form\n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23750, 20000)\n",
      "(23750, 2)\n"
     ]
    }
   ],
   "source": [
    "## transpose the X and Y sets to fit the matrix multiplication with the weights\n",
    "X, Y, X_test, Y_test = trainX.T, convert_one_hot(trainY,2).T, testX.T, convert_one_hot(testY,2).T\n",
    "#X_dev, Y_dev = devX.T, convert_one_hot(devY,2).T\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 200\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "beta= 0.01\n",
    "\n",
    "## number of nodes and classes\n",
    "n_hidden_1 = 10\n",
    "n_hidden_2 = 10\n",
    "#n_hidden_3 = 15\n",
    "n_input = X.shape[1] \n",
    "n_classes = 2 \n",
    "\n",
    "\n",
    "## input, labels, and dropout rate\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create model\n",
    "def model(x, weights, biases):\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    # Another RELU activation hidden layer\n",
    "    #layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    #layer_3 = tf.nn.relu(layer_3)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Weights and Biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    #'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    #'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = model(x, weights, biases)\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize tensors\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 5.230719658\n",
      "Epoch: 0002 cost= 0.882521030\n",
      "Epoch: 0003 cost= 0.755882105\n",
      "Epoch: 0004 cost= 0.742755489\n",
      "Epoch: 0005 cost= 0.721170555\n",
      "Epoch: 0006 cost= 0.701595811\n",
      "Epoch: 0007 cost= 0.698177796\n",
      "Epoch: 0008 cost= 0.696413666\n",
      "Epoch: 0009 cost= 0.693069766\n",
      "Epoch: 0010 cost= 0.688098441\n",
      "Epoch: 0011 cost= 0.674957937\n",
      "Epoch: 0012 cost= 0.667929894\n",
      "Epoch: 0013 cost= 0.652987272\n",
      "Epoch: 0014 cost= 0.642685183\n",
      "Epoch: 0015 cost= 0.624604298\n",
      "Epoch: 0016 cost= 0.587099088\n",
      "Epoch: 0017 cost= 0.541184148\n",
      "Epoch: 0018 cost= 0.508879813\n",
      "Epoch: 0019 cost= 0.470570026\n",
      "Epoch: 0020 cost= 0.428529594\n",
      "Epoch: 0021 cost= 0.407388719\n",
      "Epoch: 0022 cost= 0.378315630\n",
      "Epoch: 0023 cost= 0.362045878\n",
      "Epoch: 0024 cost= 0.340158748\n",
      "Epoch: 0025 cost= 0.322056137\n",
      "Epoch: 0026 cost= 0.306025258\n",
      "Epoch: 0027 cost= 0.286645289\n",
      "Epoch: 0028 cost= 0.275036930\n",
      "Epoch: 0029 cost= 0.262447769\n",
      "Epoch: 0030 cost= 0.254901927\n",
      "Epoch: 0031 cost= 0.250297344\n",
      "Epoch: 0032 cost= 0.241849230\n",
      "Epoch: 0033 cost= 0.236513975\n",
      "Epoch: 0034 cost= 0.229553719\n",
      "Epoch: 0035 cost= 0.227434458\n",
      "Epoch: 0036 cost= 0.223567037\n",
      "Epoch: 0037 cost= 0.221881794\n",
      "Epoch: 0038 cost= 0.217348999\n",
      "Epoch: 0039 cost= 0.218140300\n",
      "Epoch: 0040 cost= 0.213026130\n",
      "Epoch: 0041 cost= 0.210557016\n",
      "Epoch: 0042 cost= 0.208871615\n",
      "Epoch: 0043 cost= 0.208031590\n",
      "Epoch: 0044 cost= 0.206579102\n",
      "Epoch: 0045 cost= 0.203833847\n",
      "Epoch: 0046 cost= 0.200928359\n",
      "Epoch: 0047 cost= 0.199641699\n",
      "Epoch: 0048 cost= 0.196529199\n",
      "Epoch: 0049 cost= 0.200720306\n",
      "Epoch: 0050 cost= 0.196071564\n",
      "Epoch: 0051 cost= 0.196926652\n",
      "Epoch: 0052 cost= 0.191842921\n",
      "Epoch: 0053 cost= 0.195535004\n",
      "Epoch: 0054 cost= 0.196417922\n",
      "Epoch: 0055 cost= 0.190138138\n",
      "Epoch: 0056 cost= 0.191508063\n",
      "Epoch: 0057 cost= 0.193559350\n",
      "Epoch: 0058 cost= 0.194505896\n",
      "Epoch: 0059 cost= 0.195680040\n",
      "Epoch: 0060 cost= 0.187554290\n",
      "Epoch: 0061 cost= 0.188356171\n",
      "Epoch: 0062 cost= 0.187515478\n",
      "Epoch: 0063 cost= 0.185945447\n",
      "Epoch: 0064 cost= 0.185884877\n",
      "Epoch: 0065 cost= 0.181328577\n",
      "Epoch: 0066 cost= 0.182825916\n",
      "Epoch: 0067 cost= 0.176555178\n",
      "Epoch: 0068 cost= 0.178416203\n",
      "Epoch: 0069 cost= 0.178777576\n",
      "Epoch: 0070 cost= 0.177647883\n",
      "Epoch: 0071 cost= 0.174673300\n",
      "Epoch: 0072 cost= 0.173684337\n",
      "Epoch: 0073 cost= 0.175783023\n",
      "Epoch: 0074 cost= 0.170242564\n",
      "Epoch: 0075 cost= 0.172818038\n",
      "Epoch: 0076 cost= 0.167881852\n",
      "Epoch: 0077 cost= 0.177305218\n",
      "Epoch: 0078 cost= 0.170086058\n",
      "Epoch: 0079 cost= 0.168936785\n",
      "Epoch: 0080 cost= 0.168202272\n",
      "Epoch: 0081 cost= 0.165722400\n",
      "Epoch: 0082 cost= 0.168719937\n",
      "Epoch: 0083 cost= 0.163304904\n",
      "Epoch: 0084 cost= 0.166290116\n",
      "Epoch: 0085 cost= 0.161834073\n",
      "Epoch: 0086 cost= 0.164431704\n",
      "Epoch: 0087 cost= 0.161994408\n",
      "Epoch: 0088 cost= 0.164651539\n",
      "Epoch: 0089 cost= 0.164085702\n",
      "Epoch: 0090 cost= 0.162367246\n",
      "Epoch: 0091 cost= 0.159507771\n",
      "Epoch: 0092 cost= 0.157250276\n",
      "Epoch: 0093 cost= 0.159981233\n",
      "Epoch: 0094 cost= 0.157302861\n",
      "Epoch: 0095 cost= 0.157560612\n",
      "Epoch: 0096 cost= 0.155175247\n",
      "Epoch: 0097 cost= 0.159575964\n",
      "Epoch: 0098 cost= 0.156392224\n",
      "Epoch: 0099 cost= 0.154413148\n",
      "Epoch: 0100 cost= 0.155270618\n",
      "Epoch: 0101 cost= 0.155940383\n",
      "Epoch: 0102 cost= 0.152002434\n",
      "Epoch: 0103 cost= 0.154733649\n",
      "Epoch: 0104 cost= 0.152745055\n",
      "Epoch: 0105 cost= 0.154852825\n",
      "Epoch: 0106 cost= 0.156349230\n",
      "Epoch: 0107 cost= 0.153854216\n",
      "Epoch: 0108 cost= 0.153314340\n",
      "Epoch: 0109 cost= 0.152838377\n",
      "Epoch: 0110 cost= 0.157543933\n",
      "Epoch: 0111 cost= 0.149721641\n",
      "Epoch: 0112 cost= 0.152512446\n",
      "Epoch: 0113 cost= 0.150934520\n",
      "Epoch: 0114 cost= 0.153910971\n",
      "Epoch: 0115 cost= 0.155339776\n",
      "Epoch: 0116 cost= 0.157576825\n",
      "Epoch: 0117 cost= 0.152433370\n",
      "Epoch: 0118 cost= 0.152394591\n",
      "Epoch: 0119 cost= 0.148321828\n",
      "Epoch: 0120 cost= 0.152631736\n",
      "Epoch: 0121 cost= 0.148946646\n",
      "Epoch: 0122 cost= 0.151390267\n",
      "Epoch: 0123 cost= 0.154382190\n",
      "Epoch: 0124 cost= 0.148397050\n",
      "Epoch: 0125 cost= 0.151380747\n",
      "Epoch: 0126 cost= 0.145434537\n",
      "Epoch: 0127 cost= 0.146932908\n",
      "Epoch: 0128 cost= 0.144909337\n",
      "Epoch: 0129 cost= 0.142216109\n",
      "Epoch: 0130 cost= 0.143983531\n",
      "Epoch: 0131 cost= 0.147081198\n",
      "Epoch: 0132 cost= 0.151517057\n",
      "Epoch: 0133 cost= 0.148050540\n",
      "Epoch: 0134 cost= 0.144228852\n",
      "Epoch: 0135 cost= 0.150044970\n",
      "Epoch: 0136 cost= 0.146795022\n",
      "Epoch: 0137 cost= 0.149396805\n",
      "Epoch: 0138 cost= 0.146323507\n",
      "Epoch: 0139 cost= 0.149931277\n",
      "Epoch: 0140 cost= 0.147511124\n",
      "Epoch: 0141 cost= 0.145377053\n",
      "Epoch: 0142 cost= 0.149142349\n",
      "Epoch: 0143 cost= 0.143857719\n",
      "Epoch: 0144 cost= 0.145156469\n",
      "Epoch: 0145 cost= 0.148360969\n",
      "Epoch: 0146 cost= 0.146268528\n",
      "Epoch: 0147 cost= 0.152749108\n",
      "Epoch: 0148 cost= 0.146402639\n",
      "Epoch: 0149 cost= 0.145148726\n",
      "Epoch: 0150 cost= 0.145076744\n",
      "Epoch: 0151 cost= 0.145389344\n",
      "Epoch: 0152 cost= 0.147404404\n",
      "Epoch: 0153 cost= 0.148107808\n",
      "Epoch: 0154 cost= 0.145478171\n",
      "Epoch: 0155 cost= 0.145956971\n",
      "Epoch: 0156 cost= 0.145413452\n",
      "Epoch: 0157 cost= 0.144216718\n",
      "Epoch: 0158 cost= 0.144435328\n",
      "Epoch: 0159 cost= 0.141575869\n",
      "Epoch: 0160 cost= 0.143459090\n",
      "Epoch: 0161 cost= 0.140561623\n",
      "Epoch: 0162 cost= 0.146044913\n",
      "Epoch: 0163 cost= 0.147397433\n",
      "Epoch: 0164 cost= 0.141194008\n",
      "Epoch: 0165 cost= 0.142727704\n",
      "Epoch: 0166 cost= 0.139327156\n",
      "Epoch: 0167 cost= 0.148570790\n",
      "Epoch: 0168 cost= 0.145812207\n",
      "Epoch: 0169 cost= 0.141633588\n",
      "Epoch: 0170 cost= 0.146706735\n",
      "Epoch: 0171 cost= 0.139060650\n",
      "Epoch: 0172 cost= 0.141106056\n",
      "Epoch: 0173 cost= 0.144978828\n",
      "Epoch: 0174 cost= 0.148972465\n",
      "Epoch: 0175 cost= 0.152826573\n",
      "Epoch: 0176 cost= 0.141837567\n",
      "Epoch: 0177 cost= 0.143504567\n",
      "Epoch: 0178 cost= 0.140026789\n",
      "Epoch: 0179 cost= 0.143475090\n",
      "Epoch: 0180 cost= 0.141263980\n",
      "Epoch: 0181 cost= 0.145312158\n",
      "Epoch: 0182 cost= 0.141005488\n",
      "Epoch: 0183 cost= 0.142362517\n",
      "Epoch: 0184 cost= 0.137510791\n",
      "Epoch: 0185 cost= 0.144272909\n",
      "Epoch: 0186 cost= 0.147932617\n",
      "Epoch: 0187 cost= 0.139008007\n",
      "Epoch: 0188 cost= 0.135436701\n",
      "Epoch: 0189 cost= 0.144719749\n",
      "Epoch: 0190 cost= 0.139228054\n",
      "Epoch: 0191 cost= 0.143526569\n",
      "Epoch: 0192 cost= 0.138996642\n",
      "Epoch: 0193 cost= 0.142600229\n",
      "Epoch: 0194 cost= 0.138729089\n",
      "Epoch: 0195 cost= 0.140307803\n",
      "Epoch: 0196 cost= 0.138692352\n",
      "Epoch: 0197 cost= 0.139456708\n",
      "Epoch: 0198 cost= 0.141417232\n",
      "Epoch: 0199 cost= 0.138646478\n",
      "Epoch: 0200 cost= 0.138051316\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1817bc5208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.993811\n",
      "Test Set Accuracy: 0.8456\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    costs = []\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(len(X)/batch_size)\n",
    "        X_batches = np.array_split(X, total_batch)\n",
    "        Y_batches = np.array_split(Y, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # Optimize and get loss to display\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y, keep_prob: 0.5})\n",
    "            # Compute average loss\n",
    "            avg_loss += loss / total_batch\n",
    "        # Epoch display code inspired from Coursera\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_loss))\n",
    "        \n",
    "        costs.append(avg_loss)\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Plots the cost graph. Inspired from Udemy course code\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy of training and test sets\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Training Set Accuracy:\", accuracy.eval({x: X, y: Y, keep_prob: 1.0}))\n",
    "    print(\"Test Set Accuracy:\", accuracy.eval({x: X_test, y: Y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Method to predict sentiment of sentences using the trained weights of the neural network\n",
    "def predict_sentiment(text):\n",
    "    saver = tf.train.Saver()\n",
    "    text = text_to_vector(text).reshape(1,20000)\n",
    "    prediction = tf.equal(tf.argmax(pred,1),1)\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "        if(prediction.eval({x: text,keep_prob: 1.0})):\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Text to predict the sentiment of\n",
    "testText = \"Duke University\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f6e76cc3b3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The trained neural network works quite well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e3df992e75ae>\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Method to predict sentiment of sentences using the trained weights of the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1249\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "# The trained neural network works quite well\n",
    "prediction = predict_sentiment(testText)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
